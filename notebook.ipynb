{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import re\n",
    "from  urllib import parse\n",
    "import requests\n",
    "from selenium.webdriver import Chrome, Remote\n",
    "import selenium\n",
    "from papers_analyser import scrape_papers_with_code as navigator\n",
    "from papers_analyser.paper import Paper, get_repo\n",
    "from git import Repo as RepoLoader# gitpython\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programme\\reproduce-ml-papers\\venv\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  \"The symbol '%s' is not in this R namespace/package.\" % name\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%load_ext rpy2.ipython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driverPath = os.path.abspath(\"\") + '/drivers/' + platform.system()\n",
    "#open(driverPath)\n",
    "driver = Chrome(driverPath + \"/chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load index page\n",
    "Infinite scroll down all the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index_page = navigator.get_paper_index_page(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get paper links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "paper_links = navigator.get_papers(index_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parse paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird behaviour of paperswithcode.com: Mixing on-site links with github links, which makes absolutely no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repos = list()\n",
    "papers = list()\n",
    "\n",
    "for paper_link in paper_links[:5]:\n",
    "    if not \"github\" in paper_link:\n",
    "        papers.append(Paper(paper_link))\n",
    "    else:\n",
    "        repos.append(get_repo(paper_link))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Take a look on README files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "readmes = [paper.repo[0].readme for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# BeyondMeSH\n",
       "### A weakly supervised approach for fine-grained semantic indexing of biomedical literature\n",
       "\n",
       "This repository includes the source code and some data for the development of models for fine-grained semantic indexing of biomedical articles.\n",
       "In particular, it includes:\n",
       "1. The folder **Data** with dataset files for the Alzheimer's Disease (AD) and Duchenne Muscular Dystrophy (DMD) use cases. These files are required to develop the the datasets for weakly-supervised fine-grained semantic indexing models. For a detailed description of these files see the corresponding README file.\n",
       "    * To avoid the data processing steps and directly develop the models, some pre-processed datasets are also available as a single zipped file and as separate files for selective download of specific sub-folders or files:\n",
       "        * For the AD use case the zipped and unzipped pre-processed datasets are available [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/AfEgLnVPoD2mDfO) (1.6 Gb) and [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/mNQvA9kN9d0xItD) (4.43 Gb) respectively. \n",
       "        * For the DMD use case the zipped and unzipped pre-processed datasets are available [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/cQvY1reNrFdYSZF) (125.6 Mb) and [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/8LynUN0y3mJVgpW) (321 Mb) respectively. \n",
       "2. The scripts **Pipeline.py** and **DatasetFunctions.py** for processing the initial dataset files to develop weakly supervised models for fine-grained semantic indexing. \n",
       "3. The **requirements.txt** file with the libraries and versions required for running the scripts.\n",
       "\n",
       "## How to use\n",
       "\n",
       "### Requirements\n",
       "The script is written in Python 3.6.\n",
       "\n",
       "All libraries and versions required are listed in requirements.txt.\n",
       "\n",
       "Memory requirements: For big datasets (e,g, for the AD use case), some steps of the pipeline (e.g. Feature Selection) can be very demanding in terms of memory. The experiments with the AD datasets provided in the \"Data\" folder have been executed in a system with more than 100Gb of memory. For less memory-demanding experiments please use smaller datasets (e.g. DMD or under-sampled AD).\n",
       "\n",
       "### Configure\n",
       " Update specific configurations in the scripts:\n",
       " * In **DatasetFunctions.py** update:\n",
       "    * (required) The **path_Separator** variable depending on your system. The default value is '/' for Unix systems.\n",
       "    * (optional) Other variables (e.g. document_fn, label_fn, noClass) to customize the names of saved files etc. Details for each variable available in the corresponding comments.\n",
       " * In **Pipeline.py** update:\n",
       "\t* (required) The **Use-Case** variables. These variebles indicate on wich use case we are experimenting. For the reported AD and DMD experiments the values are already in the script, un-commenting the correct ones is enough.\n",
       "\t\t* *caseStudy*: An abbreviation for the use case (e.g. \"AD\" or \"DMD\"). Used for folder naming etc. \n",
       "\t\t* *dominantLabel*: The UMLS CUI of the prefered concept for this uce case. Ignored in MA2 creation as well as in some baselines.\n",
       "\t\t* *cTop*: The UMLS CUI that is higher in the Ct Hierarcy for this uce case. Ignored for modeling and evaluation.\n",
       "    * (required) The **baseFolder** variable. The value of this variable should be the path to the folder with the datasets in your system. \n",
       "        * To run the experiment from the beginning use the path to the accompanying folder \"Data\". \n",
       "        * To skip the data processing steps, using the pre-processed datasets, and go directly to model development, use the path to corresponding folder (e.g. \"LexicalAndSemanticFeatures\" etc).\n",
       "    * (required) The **Steps-to-do** variables. These boolean variables define which steps of the process will be executed or skipped:\n",
       "        * To run the experiment from the beginning update all these variables to have value 1.\n",
       "        * To skip the data processing steps, using the pre-processed datasets, and go directly to model development update all these variables to have value 0, except from \"classify\" which should have value 1.\n",
       "\t* (required) The **Feature-Selection** variables. These variables define the alternative feature selection configurations to be considered. \n",
       "\t\t* The default values correspond to the pre-processed files provided. If you add new values here, steps 3 and 5 must be excecuted before step 6 for modeling.\n",
       "\t\t* *featureKs*: Alternative Numbers of features to select in Feature selection.\n",
       "\t\t* *scoreFunctions*: Alternative score functions to be used for Univariate feature selection.\n",
       "\t* (optional) The **test_folder_names** variable. Defines which testsets will be used for evaluation. \n",
       "        * Set to ['MA1'] to use the consensus manual annotations for the randomly selected MA1 dataset.        \n",
       "        * Set to ['MA2'] to use the consensus manual annotations for the weak balanced MA2 dataset.\n",
       "        * The default is ['MA1','MA2'] to use both. \n",
       "\t* (optional) Naming conventions variables (e.g. class_csv, final_dir, feature_prefix etc) to customize the names of saved files etc. Details for each variable available in the corresponding comments.\n",
       "\t\t* The default values correspond to the pre-processed files provided. \n",
       "\t* (optional) **Step 1** variables: Configure dataset creation\n",
       "\t\t* The default values correspond to the pre-processed files provided. \n",
       "\t\t* *manual_dataset1_pmids* and *manual_dataset2_pmids*: Path to files with the pmids of the MA1 and MA2 datasets to be removed from the training dataset creation. If no MA1 and MA2 datasets are available, set to False.\n",
       "\t\t* *manual_dataset_1_size* and *manual_dataset_1_size*: How many articles to select for creation of new MA1 and MA2 datasets. If MA1 and MA2 datasets are already selected set to 0.\n",
       "\t\t* *majority_articles_subsample_size*: Number of \"preferred class (only)\" articles to be removed from the trainig dataset for undesampling. If 0, no under-sampling is performed.\n",
       "\t* (optional) **Step 2 & 3** variables: Configure feature types considered\n",
       "        * If you skip the data processing steps, using the pre-processed datasets, this variable has no effect.         \n",
       "\t\t* *useCUIS*: When *true* consider semantic features (concept occurrence), when *false* only lexical. \n",
       "\t\t* *binaryFrequency*:  When *true* consider binary semantic features (concept occurs or not), when *false* use absolute frequency of concept occurrences. \n",
       "\t\t* *ignoreLabelFeatures*:  When *true* exclude concepts used for weak supervision (*ci*) from the feature representation. \t\t\n",
       "\t* (optional) **Step 6** variables: Configure model training\n",
       "\t\t* The default values correspond to the pre-processed files provided.\n",
       "\t\t* *clfTypes*: A dictionary with alternative classifier types to be trained\n",
       "\t\t* *cvs*: A list with alternative numbers of folds for cross-validation on training dataset\n",
       "\t\t* *regCs*: A list with alternative values for Logistic Regression Classifier regularization levels (parameter C) \n",
       "\t\t* *regType*: The type of regularization (\"l2\" or \"l1\") to be performed in Logistic Regression Classifier models     \n",
       "\t\t\n",
       "### Run\n",
       "\n",
       "Example call:\n",
       "\n",
       "> python3.6 Pipeline.py\n",
       "\n",
       "The results will be stored in this folder: \n",
       "\n",
       "> *baseFolder*\\FinalSplit\\\\*UseCase*\n",
       "\n",
       "Where *baseFolder* is the absolute path provided in the configuration above and *UseCase* is the abbreviation of the use case (i.e. AD or DMD). \n",
       "\n",
       "* Execution of model development and evaluation step should result in the creation of corresponding CSV files with performance metrics for the models developed. The CSV files will be named **ScoresPerDataset_*D*_*C*.csv** and **socresPerDatasetPerLabel_*D*_*C*.csv**, where *D* can take values 'MA1' or 'MA2' depending on the dataset configured as testset and *C* is the regularization level for Logistic Regression models (e.g. ScoresPerDataset_MA1_1.csv and socresPerDatasetPerLabel_MA1_1.csv).\n",
       "    * For iteration experiments the corresponding CSV files will be named **ScoresPerDataset_*D*\\_*C*\\_*I*.csv** and **socresPerDatasetPerLabel_*D*\\_*C*\\_*I*.csv**, where *I* will be the number of corresponding iteration (i.e. ScoresPerDataset_MA1_1_0.csv for the first iteration, ScoresPerDataset_MA1_1_1.csv for the second one etc).\n",
       "* Execution of data processing steps should result in the creation of corresponding intermediate files. (Like the pre-processed ones provided [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/cQvY1reNrFdYSZF) and [here](https://owncloud.skel.iit.demokritos.gr/index.php/s/AfEgLnVPoD2mDfO) \n",
       "\n",
       "## Reference\n",
       "\n",
       "Nentidis, A., Krithara, A., Tsoumakas, G., & Paliouras, G. (2019). Beyond MeSH: Fine-Grained Semantic Indexing of Biomedical Literature Based on Weak Supervision. In 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS) (pp. 180–185). IEEE. https://doi.org/10.1109/CBMS.2019.00045\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(readmes[4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir = driverPath = os.path.abspath(\"\") + \"/repos\"\n",
    "for paper in papers:\n",
    "    paper_path = dir + \"/\" + paper.title.replace( \":\",\"\")\n",
    "    if not os.path.exists(paper_path):\n",
    "        os.mkdir(paper_path)\n",
    "        \n",
    "    for repo in paper.repo:\n",
    "        repo_path = paper_path+ repo.repo_name.replace(\"/\",\"_\")\n",
    "        \n",
    "        if not os.path.exists(repo_path):\n",
    "            os.mkdir(repo_path)\n",
    "            \n",
    "        RepoLoader.clone_from(repo.clone_url,repo_path)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}